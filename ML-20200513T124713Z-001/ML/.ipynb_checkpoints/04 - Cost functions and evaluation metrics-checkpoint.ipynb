{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions and evaluation metrics\n",
    "## How are the algorithms trained?\n",
    "We already established that our goal is to create an estimator $\\hat{F}(X)$, that tries to approximate a true and unobservable function $F(X)$. In supervised learning we use a training set for which true values are given as $y$. For the same set of data our estimator gives predictions $\\hat{y}$ and therefore we can calculate the prediction error $y-\\hat{y}$.\n",
    "\n",
    "There are many algorithms for finding $\\hat{F}(X)$. Every one of them uses a different approach or recipe to generate the estimator. However, no matter which algorithm we use and how we go about construction of $\\hat{F}(X)$ one thing remains constant. We want the prediction error to be as small as possible.\n",
    "\n",
    "With this goal in mind we chose a cost function also called a loss function. This framing is easy to remember as we want our error to be as low as possible and hence we want to minimize cost/error/loss.\n",
    "\n",
    "When we say we want our estimator to be as good as possible we need to optimize it. Optimization is nothing else but minimization of the cost function.\n",
    "\n",
    "No matter what type of algorithm we use, weather there is an analytical solution or only a numerical approximation we always optimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the cost function\n",
    "Choosing a right function is crucial as the cost function **directly affects the our estimator $\\hat{F}(X)$**. Therefore, whenever possible, we will choose a function for which it is possible to prove that our estimator is unbiased - its parameters are equal to true values.\n",
    "\n",
    "$$E(\\hat\\beta)=\\beta $$\n",
    "\n",
    "In best case scenario we would also like for the loss function to create an *minimum-variance unbiased estimator (MVUE)*.\n",
    "\n",
    "Last but not least, as optimization algorithms relay on differentiation we want the cost function to be smooth. When our estimator doesn't have this property our (numerical) optimizer may not converge at all or stop in some local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic cost functions\n",
    "### Mean Squared Error (MSE)\n",
    "One of the most commonly used cost functions is *mean squared error (MSE)*. Well known metric used in most basic econometric method of linear regression *ordinary least squares (OLS)*.\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2 $$\n",
    "The reason for this method popularity is tha fact that this estimator is unbiased and efficient.\n",
    "\n",
    "Those advantages do not meant that we can call MSE \"the best cost function\". One of its biggest shortcomings is the fact that it puts quite a lot of weight towards outliers.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "One solution for over-weighting of outliers is using mean absolute error.\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y_i}| $$\n",
    "However this is also not a perfect function, mainly due to the fact that it is not differentialble in zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of loss functions\n",
    "In practical applications of machine learning we should never forget why we are engaged in predictive modeling. What is the *real* or business meaning of prediction errors? What does it mean for use, business wise, that we made a prediction error. Do we really make twice as big of a mistake when the prediction error increases two fold? Maybe it is the opposite? Maybe the bigger the prediction error the less important it is?\n",
    "\n",
    "We can also easily imagine a situation in which the importance of our prediction error would be asymmetric. For instance in credit scoring predicting too low is not as bad as predicting too high. Similarly making a type I error can be better/worse than making type II error. In these situations we can create a custom asymmetrical cost function to incorporate this asymmetry.\n",
    "\n",
    "To sum up a chosen cost function should always **reflect the real cost of prediction errors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function vs evaluation metrics\n",
    "In theory it is possible to apply almost any cost function to most ML algorithms. However in most cases it would not be a successful implementation (mainly due to function being non-smooth). In practice unless there is a very strong business need for custom cost function, a basic function would be used. However we widen our knowledge about estimators using additional evaluation metrics.\n",
    "\n",
    "We calculate evaluation metrics after the estimator is already created with use of different cost function. The **evaluation metrics do not affect the form of estimator $\\hat{F}(X)$**.\n",
    "\n",
    "Evaluation functions are usually called metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in regression\n",
    "In regression problems most common metrics are:\n",
    "* MSE - Mean Square Error\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2 $$\n",
    "* MAE - Mean Absolute Error\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y_i}| $$\n",
    "* MedAE - Median Absolute Error - this metric is immune to over-weighting of outliers.\n",
    "$$MedAE = median( |y_0 - \\hat{y_0}| \\dots |y_n - \\hat{y_n}| )$$\n",
    "* MSLE - Mean Squared Logarithmic Error - and example of an asymmetric metric the bigger the error the, relatively, lower the wieght.\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (\\log(1+y_i) - \\log(1+\\hat{y_i}))^2 $$\n",
    "* $R^2$ - Coefficient of determination\n",
    "$$R^2 \\equiv 1 - {RSS \\over TSS}$$\n",
    "where:\n",
    "$$\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i$$\n",
    "$$RSS=\\sum_i (y_i-\\hat{y_i})^2$$\n",
    "$$TSS=\\sum_i (y_i-\\bar{y})^2$$\n",
    "\n",
    "In case of regression problems the interpretation of the distribution of prediction error is straightforward. Most people intuitively understand what is mean square error. In any time we can plot a histogram of our prediction errors and we will be able to get a complete picture of the performance of regression estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in classification\n",
    "On the other hand in classification problems it is not that easy to asses the prediction error, especially when by predictions we understand binary labels. In this case the first measure that comes to mind is accuracy. Lets see how our predictions can look like in binary classification.\n",
    "\n",
    "![CONFMATRIX](img/confMatrix2.png)\n",
    "Źródło: http://www.ritchieng.com/machine-learning-evaluate-classification-model/\n",
    "\n",
    "Symbols:\n",
    "$TP$ - true positive, $TN$ - True negatives, $FP$ - False positives, $FN$ - False negatives, $P$ - number of positives, $N$ - number of negatives\n",
    "\n",
    "* Acc - Accuracy\n",
    "$$ Acc = {(TP+TN) \\over (P+N)}$$\n",
    "It should be quite clear that accuracy is not a very good metric. For well balanced data sets we intuitive interpretation of accuracy is correct. however of imbalanced datasets accuracy can quickly become almost useless. Imagine a dataset in which there are ninety-nine \"1\" and one \"0\". We can get 99% accuracy by simply assigning \"1\" to every observation. Balanced accuracy tries to alleviate this problem.\n",
    "\n",
    "* Bacc - Balanced Accuracy\n",
    "$$ Bacc = ({TP \\over P} + {TN \\over N})/2$$\n",
    "\n",
    "When we look ar \"ones\" and \"zeros\" together we can get a good picture of the performance of our estimator. We can try to look what share of labels did we get right:\n",
    "* TPR - True Positive Rate - Sensitivity, Hit Rate, Recall\n",
    "$$ TPR = {TP \\over P} = {TP \\over TP+FN}$$\n",
    "* TNR (SPC) - True Negative Rate - Specificity\n",
    "$$TNR = {TN \\over N} = {TN \\over FP+TN}$$\n",
    "\n",
    "Another approach is to use total number of predictions in current group\n",
    "* PPV - Positive Predictive Value - Precision\n",
    "$$ PPV = {TP \\over TP + FP}$$\n",
    "* NPV - Negative Predictive Value \n",
    "$$ NPV = {TN \\over TN+FN}$$\n",
    "\n",
    "From time to time we can also see a following metric:\n",
    "* F1 - f-score\n",
    "$$F_1 = 2 * {(precision * recall) \\over (precision + recall)}$$\n",
    "\n",
    "A table below is a nice summary of classification metrics:\n",
    "![measureMatrix](img/measuresMatrix.png)\n",
    "Source: https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in classification - cost functions and probabilities\n",
    "### Logloss, cross entropy\n",
    "When we look at our prediction in terms of labels we lose a very important information - what probability did the estimator assign to a given observation. All ML algorithms \"work\" using probabilities, rational numbers between 0 and 1, and not labels. Only at the end we chose a cut-off point and assign labels based on probabilities. For one of the \"1\" the probability can be equal to -.55 and it can be 0.87 for other. We can say that in the second case the algorithm was more certain. We lose this information while working with just labels.\n",
    "\n",
    "So lets take a look at the most important cost functions in classification. in logistic regression we were optimizing probabilities by maximizing the likelihood of observing the data in training set. In practice due to the shape of logistic function our cost function was given as:\n",
    "$$f^*_\\text{Logistic}= \\ln\\left(\\frac{p(1\\mid x)}{1-p(1\\mid x)}\\right) = \\ln\\left(p(1\\mid x)\\right)-\\ln\\left(1-p(1\\mid x)\\right)$$\n",
    "\n",
    "Now lets take a look at the equation for *cross entropy* in binary problem for which the cost function will look like this:\n",
    "\n",
    "$$J = -\\frac1n\\sum_{i=1}^n\\ \\bigg[y_i  \\log \\hat y_i + (1 - y_i)  \\log (1 - \\hat y_i)\\bigg]$$\n",
    "As you can see in binary classification either first or second term of the sum will be equal to zero. As our true values are either 0 or 1 we will have either $\\log \\hat y_i$ or $\\log (1 - \\hat y_i)$. Therefore the algorithm will try to get as close as possible to true probability. One can say that this function is very similar to MSLE. \n",
    "\n",
    "Whats more we can very simply generalize this function to a multinominal problem:\n",
    "\n",
    "$$J = - \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{i,k} \\log \\hat y_{i,k}$$\n",
    "\n",
    "In this case we cannot do a p, 1-p \"trick\" so we will just look at the log of the probability of a true class. It will be zero for all other classes. This equation is usually called multinominal logloss. Main advantage is its smoothness.\n",
    "\n",
    "\n",
    "### Hinge loss\n",
    "In case of logloss the value of cost function changes continuously no matter weather we make good or bad predictions. Hinge loss behaves differently in that respect. We assume that our target values ar -1 or 1. Than for one observation we have\n",
    "$$l(y) = \\max(0, 1-y \\cdot \\hat y)$$\n",
    "As you can see in hinge loss our loss is zero when we assign high enough probability. Therefore the algorithm focuses only on the \"harder\" observations to separate. This loss function is used in a fairly important algorithm *Support Vecotr Machine (SVM)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for probabilities\n",
    "In case of classification we do not usually use logloss nor hing loss for classification. It is very hard to interpret their values directly and almost impossible to compare between datasets. Instead we use one of the following metrics.\n",
    "\n",
    "### ROC\n",
    "* ROC - Receiver Operating Characteristic Curve\n",
    "* AUC ROC - Area Under Curve - It is the area under curve ROC. Usually when we see just AUC it means AUC ROC. It takes values from 0.5 to1. In case where we want to describe a quality of binary classification with just one number it is one of the best metrics.\n",
    "![ROC](img/roc.png)\n",
    "Source: http://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "ROC tells us how the values of TPR and FPR (FPR = 1- TNR) change with the rise of the cut off point. As you can see the curve always go to the corners of the square. At the begning when the cutoff point is 1 we classify every observation as \"0\". Obviously in this situation TNR = 1 (FPR  = 0). With the decrease of the cutoff point we increase the number of \"1\" - the TPR starts to increase. However our estimator will probably not be perfect so some of predicted \"1\" are incorrect, therefore the increase of FPR (and decrease of TNR).\n",
    "\n",
    "A perfect estimator would go from lower-bottom corner to top-right through top-left corner.\n",
    "\n",
    "### Lift \n",
    "Lift is strictly connected to AUC. It focuses on relative performance to random estimator. usually we plot Lift curve for deciles. One value tells the ratio of correctly predicted ones by our estimator in comparison to random estimator. A value 2.5 for second decil means that wen we chose 20% of predictions with highest probabilities we predicted 50% of all \"1\" (TPR = 0.5)\n",
    "\n",
    "![LIFT](img/lift.png)\n",
    "Source: https://www.neuraldesigner.com/blog/methods-binary-classification \n",
    "The values of Lift we can read from ROC by looking vertically and comparing the value of ROC to the diagonal.\n",
    "\n",
    "### Precision/recall curve - AUC PR\n",
    "The last metric, that is recently gaining in popularity, is precision/recall curve. On one hand it is very similar to ROC as we use the same points (ordered probabilities). However instead of using TPR and TNR we look ad TPR (Recall) and PPV (Precision).\n",
    " \n",
    "<img src=\"img/precRecallCurve.png\" width=\"45%\">\n",
    "Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "\n",
    "(moj komentarz) - 40% będzie jedynek ze 100% pewnoscią\n",
    "\n",
    "In most cases there will be little difference in the quality of our model and ranking order of models between AUC ROC and AUC PR. however when the data set is heavily unbalanced we should chose our metrics very carefully. The research shows that we can get very peculiar behavior. For instance in figure below we see two algorithms working on dataset with 20 \"1\" and 2000 \"0\". The AUC ROC values are 0.813 for algorithm  I and 0.875 for II, and AUC PR are  0.51 and 0.03 respectively.\n",
    "\n",
    "<img src=\"img/ROCPRREC.png\">\n",
    "Source: Davis, J., & Goadrich, M. (2006, June). The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd international conference on Machine learning (pp. 233-240). ACM.\n",
    "\n",
    "\n",
    "(moj kom) - left plot - very very rare that two curves are crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Excercise\n",
    "Lets go back to our logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:57:50.959211Z",
     "start_time": "2020-05-03T13:57:44.466593Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-ticks')\n",
    "%matplotlib inline\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:57:51.014643Z",
     "start_time": "2020-05-03T13:57:50.961269Z"
    }
   },
   "outputs": [],
   "source": [
    "medical = pd.read_pickle(\"data/medical.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:57:51.026401Z",
     "start_time": "2020-05-03T13:57:51.017422Z"
    }
   },
   "outputs": [],
   "source": [
    "# The file includes socio-demographic data, including \n",
    "# health insurance and various aspects of health care\n",
    "# touchpoints for the respondent group of a survey\n",
    "# conducted in the USA.\n",
    "\n",
    "# The collection includes 35072 observations and 27 variables:\n",
    "  \n",
    "# UMARSTAT – Marital status recode\n",
    "# UCUREMP – Currently has employer coverage\n",
    "# UCURNINS – Currently uninsured\n",
    "# USATMED – Satisfied with quality of medical care\n",
    "# URELATE – Number of relatives in household\n",
    "# REGION – region\n",
    "# STATE - state\n",
    "# HHID – Household identification number\n",
    "# FHOSP – In hospital overnight last year\n",
    "# FDENT – Dental visits last year\n",
    "# FEMER – Number of emergency room visits last year\n",
    "# FDOCT – Number of doctor visits last year\n",
    "# UIMMSTAT – Immigration status\n",
    "# U_USBORN – U.S.- or foreign-born\n",
    "# UAGE – Age topcoded\n",
    "# U_FTPT – Full-time or part-time worker this year\n",
    "# U_WKSLY – Weeks worked last year\n",
    "# U_HRSLY – Hours worked per week last year\n",
    "# U_USHRS – Hours worked per week this year\n",
    "# HEARNVAL – Earnings amount last year - Household\n",
    "# HOTHVAL – Household income, total exc. earnings\n",
    "# HRETVAL – Retirement amount – Household\n",
    "# HSSVAL – Social Security amount - Household\n",
    "# HWSVAL – Wages and salaries amount – Household\n",
    "# UBRACE – race\n",
    "# GENDER – gender\n",
    "# UEDUC3 – education level\n",
    "# CEYES - color of eyes\n",
    "# CHAIR - color of hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:57:57.745807Z",
     "start_time": "2020-05-03T13:57:55.380332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>U_USHRS</td>     <th>  R-squared:         </th>  <td>   0.822</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.822</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   4142.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 03 May 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:57:57</td>     <th>  Log-Likelihood:    </th> <td>-1.1026e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 35072</td>      <th>  AIC:               </th>  <td>2.206e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 35032</td>      <th>  BIC:               </th>  <td>2.209e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    39</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td>   11.2403</td> <td>    0.410</td> <td>   27.401</td> <td> 0.000</td> <td>   10.436</td> <td>   12.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married, do not live together]</th> <td>    0.8303</td> <td>    0.352</td> <td>    2.358</td> <td> 0.018</td> <td>    0.140</td> <td>    1.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married_live together]</th>         <td>    0.0955</td> <td>    0.103</td> <td>    0.923</td> <td> 0.356</td> <td>   -0.107</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Never married]</th>                 <td>    0.5790</td> <td>    0.122</td> <td>    4.732</td> <td> 0.000</td> <td>    0.339</td> <td>    0.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Partnership]</th>                   <td>    0.0500</td> <td>    0.185</td> <td>    0.270</td> <td> 0.787</td> <td>   -0.312</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Separated]</th>                     <td>    0.2791</td> <td>    0.199</td> <td>    1.405</td> <td> 0.160</td> <td>   -0.110</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Unknown]</th>                       <td>   -0.6140</td> <td>    0.808</td> <td>   -0.760</td> <td> 0.447</td> <td>   -2.197</td> <td>    0.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Widowed]</th>                       <td>    0.5696</td> <td>    0.263</td> <td>    2.166</td> <td> 0.030</td> <td>    0.054</td> <td>    1.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UCUREMP[T.Yes]</th>                            <td>   -0.2474</td> <td>    0.076</td> <td>   -3.237</td> <td> 0.001</td> <td>   -0.397</td> <td>   -0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.Northeast]</th>                       <td>   -0.1911</td> <td>    0.087</td> <td>   -2.207</td> <td> 0.027</td> <td>   -0.361</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.South]</th>                           <td>   -0.0997</td> <td>    0.082</td> <td>   -1.216</td> <td> 0.224</td> <td>   -0.260</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.West]</th>                            <td>   -0.1319</td> <td>    0.084</td> <td>   -1.574</td> <td> 0.116</td> <td>   -0.296</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FHOSP[T.Yes]</th>                              <td>   -0.1287</td> <td>    0.119</td> <td>   -1.079</td> <td> 0.280</td> <td>   -0.362</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.Foreign-born, non-citizen]</th>     <td>   -0.3991</td> <td>    0.183</td> <td>   -2.180</td> <td> 0.029</td> <td>   -0.758</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.US-born citizen]</th>               <td>    0.2965</td> <td>    0.153</td> <td>    1.941</td> <td> 0.052</td> <td>   -0.003</td> <td>    0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_FTPT[T.Part-time]</th>                       <td>   -7.5472</td> <td>    0.100</td> <td>  -75.332</td> <td> 0.000</td> <td>   -7.744</td> <td>   -7.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Asian/Pacific Islander]</th>          <td>    0.0153</td> <td>    0.320</td> <td>    0.048</td> <td> 0.962</td> <td>   -0.612</td> <td>    0.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Black]</th>                           <td>    0.0104</td> <td>    0.274</td> <td>    0.038</td> <td> 0.970</td> <td>   -0.527</td> <td>    0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.White]</th>                           <td>    0.3048</td> <td>    0.270</td> <td>    1.129</td> <td> 0.259</td> <td>   -0.224</td> <td>    0.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.Male]</th>                            <td>    0.5987</td> <td>    0.066</td> <td>    9.014</td> <td> 0.000</td> <td>    0.469</td> <td>    0.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(UEDUC3)[T.1]</th>                            <td>    0.2516</td> <td>    0.105</td> <td>    2.392</td> <td> 0.017</td> <td>    0.045</td> <td>    0.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(UEDUC3)[T.2]</th>                            <td>    0.6068</td> <td>    0.116</td> <td>    5.238</td> <td> 0.000</td> <td>    0.380</td> <td>    0.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CEYES[T.brown]</th>                            <td>    0.0438</td> <td>    0.085</td> <td>    0.516</td> <td> 0.606</td> <td>   -0.122</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CEYES[T.green]</th>                            <td>    0.1628</td> <td>    0.113</td> <td>    1.441</td> <td> 0.150</td> <td>   -0.059</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CEYES[T.hazel]</th>                            <td>   -0.1141</td> <td>    0.103</td> <td>   -1.107</td> <td> 0.268</td> <td>   -0.316</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAIR[T.blond]</th>                            <td>    0.0843</td> <td>    0.114</td> <td>    0.739</td> <td> 0.460</td> <td>   -0.139</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAIR[T.brown]</th>                            <td>   -0.0475</td> <td>    0.091</td> <td>   -0.522</td> <td> 0.601</td> <td>   -0.226</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAIR[T.red]</th>                              <td>   -0.0966</td> <td>    0.123</td> <td>   -0.786</td> <td> 0.432</td> <td>   -0.338</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UAGE</th>                                      <td>   -0.0109</td> <td>    0.003</td> <td>   -3.422</td> <td> 0.001</td> <td>   -0.017</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_WKSLY</th>                                   <td>   -0.0094</td> <td>    0.003</td> <td>   -2.883</td> <td> 0.004</td> <td>   -0.016</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_HRSLY</th>                                   <td>    0.7620</td> <td>    0.003</td> <td>  258.890</td> <td> 0.000</td> <td>    0.756</td> <td>    0.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HEARNVAL</th>                                  <td>-1.488e-07</td> <td> 1.71e-06</td> <td>   -0.087</td> <td> 0.930</td> <td>-3.49e-06</td> <td> 3.19e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HOTHVAL</th>                                   <td>-6.187e-06</td> <td> 2.67e-06</td> <td>   -2.314</td> <td> 0.021</td> <td>-1.14e-05</td> <td>-9.47e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HRETVAL</th>                                   <td> 9.114e-06</td> <td> 5.77e-06</td> <td>    1.580</td> <td> 0.114</td> <td>-2.19e-06</td> <td> 2.04e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSSVAL</th>                                    <td> 7.999e-06</td> <td> 6.46e-06</td> <td>    1.237</td> <td> 0.216</td> <td>-4.67e-06</td> <td> 2.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HWSVAL</th>                                    <td> 4.514e-07</td> <td> 1.79e-06</td> <td>    0.252</td> <td> 0.801</td> <td>-3.06e-06</td> <td> 3.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDENT</th>                                     <td>    0.0038</td> <td>    0.021</td> <td>    0.186</td> <td> 0.853</td> <td>   -0.037</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FEMER</th>                                     <td>    0.0336</td> <td>    0.041</td> <td>    0.823</td> <td> 0.411</td> <td>   -0.046</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDOCT</th>                                     <td>   -0.0158</td> <td>    0.009</td> <td>   -1.775</td> <td> 0.076</td> <td>   -0.033</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>URELATE</th>                                   <td>    0.0827</td> <td>    0.024</td> <td>    3.417</td> <td> 0.001</td> <td>    0.035</td> <td>    0.130</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>15710.592</td> <th>  Durbin-Watson:     </th>  <td>   1.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>794998.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.400</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>26.156</td>   <th>  Cond. No.          </th>  <td>2.57e+06</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.57e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                U_USHRS   R-squared:                       0.822\n",
       "Model:                            OLS   Adj. R-squared:                  0.822\n",
       "Method:                 Least Squares   F-statistic:                     4142.\n",
       "Date:                Sun, 03 May 2020   Prob (F-statistic):               0.00\n",
       "Time:                        15:57:57   Log-Likelihood:            -1.1026e+05\n",
       "No. Observations:               35072   AIC:                         2.206e+05\n",
       "Df Residuals:                   35032   BIC:                         2.209e+05\n",
       "Df Model:                          39                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================================\n",
       "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                    11.2403      0.410     27.401      0.000      10.436      12.044\n",
       "UMARSTAT[T.Married, do not live together]     0.8303      0.352      2.358      0.018       0.140       1.520\n",
       "UMARSTAT[T.Married_live together]             0.0955      0.103      0.923      0.356      -0.107       0.298\n",
       "UMARSTAT[T.Never married]                     0.5790      0.122      4.732      0.000       0.339       0.819\n",
       "UMARSTAT[T.Partnership]                       0.0500      0.185      0.270      0.787      -0.312       0.412\n",
       "UMARSTAT[T.Separated]                         0.2791      0.199      1.405      0.160      -0.110       0.668\n",
       "UMARSTAT[T.Unknown]                          -0.6140      0.808     -0.760      0.447      -2.197       0.969\n",
       "UMARSTAT[T.Widowed]                           0.5696      0.263      2.166      0.030       0.054       1.085\n",
       "UCUREMP[T.Yes]                               -0.2474      0.076     -3.237      0.001      -0.397      -0.098\n",
       "REGION[T.Northeast]                          -0.1911      0.087     -2.207      0.027      -0.361      -0.021\n",
       "REGION[T.South]                              -0.0997      0.082     -1.216      0.224      -0.260       0.061\n",
       "REGION[T.West]                               -0.1319      0.084     -1.574      0.116      -0.296       0.032\n",
       "FHOSP[T.Yes]                                 -0.1287      0.119     -1.079      0.280      -0.362       0.105\n",
       "UIMMSTAT[T.Foreign-born, non-citizen]        -0.3991      0.183     -2.180      0.029      -0.758      -0.040\n",
       "UIMMSTAT[T.US-born citizen]                   0.2965      0.153      1.941      0.052      -0.003       0.596\n",
       "U_FTPT[T.Part-time]                          -7.5472      0.100    -75.332      0.000      -7.744      -7.351\n",
       "UBRACE[T.Asian/Pacific Islander]              0.0153      0.320      0.048      0.962      -0.612       0.643\n",
       "UBRACE[T.Black]                               0.0104      0.274      0.038      0.970      -0.527       0.548\n",
       "UBRACE[T.White]                               0.3048      0.270      1.129      0.259      -0.224       0.834\n",
       "GENDER[T.Male]                                0.5987      0.066      9.014      0.000       0.469       0.729\n",
       "C(UEDUC3)[T.1]                                0.2516      0.105      2.392      0.017       0.045       0.458\n",
       "C(UEDUC3)[T.2]                                0.6068      0.116      5.238      0.000       0.380       0.834\n",
       "CEYES[T.brown]                                0.0438      0.085      0.516      0.606      -0.122       0.210\n",
       "CEYES[T.green]                                0.1628      0.113      1.441      0.150      -0.059       0.384\n",
       "CEYES[T.hazel]                               -0.1141      0.103     -1.107      0.268      -0.316       0.088\n",
       "CHAIR[T.blond]                                0.0843      0.114      0.739      0.460      -0.139       0.308\n",
       "CHAIR[T.brown]                               -0.0475      0.091     -0.522      0.601      -0.226       0.131\n",
       "CHAIR[T.red]                                 -0.0966      0.123     -0.786      0.432      -0.338       0.144\n",
       "UAGE                                         -0.0109      0.003     -3.422      0.001      -0.017      -0.005\n",
       "U_WKSLY                                      -0.0094      0.003     -2.883      0.004      -0.016      -0.003\n",
       "U_HRSLY                                       0.7620      0.003    258.890      0.000       0.756       0.768\n",
       "HEARNVAL                                  -1.488e-07   1.71e-06     -0.087      0.930   -3.49e-06    3.19e-06\n",
       "HOTHVAL                                   -6.187e-06   2.67e-06     -2.314      0.021   -1.14e-05   -9.47e-07\n",
       "HRETVAL                                    9.114e-06   5.77e-06      1.580      0.114   -2.19e-06    2.04e-05\n",
       "HSSVAL                                     7.999e-06   6.46e-06      1.237      0.216   -4.67e-06    2.07e-05\n",
       "HWSVAL                                     4.514e-07   1.79e-06      0.252      0.801   -3.06e-06    3.96e-06\n",
       "FDENT                                         0.0038      0.021      0.186      0.853      -0.037       0.044\n",
       "FEMER                                         0.0336      0.041      0.823      0.411      -0.046       0.114\n",
       "FDOCT                                        -0.0158      0.009     -1.775      0.076      -0.033       0.002\n",
       "URELATE                                       0.0827      0.024      3.417      0.001       0.035       0.130\n",
       "==============================================================================\n",
       "Omnibus:                    15710.592   Durbin-Watson:                   1.972\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           794998.681\n",
       "Skew:                           1.400   Prob(JB):                         0.00\n",
       "Kurtosis:                      26.156   Cond. No.                     2.57e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.57e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = sm.OLS.from_formula(formula='U_USHRS ~ UAGE + U_WKSLY + U_HRSLY + HEARNVAL + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + FDENT + FEMER + FDOCT + UMARSTAT + UCUREMP + URELATE + REGION + FHOSP + UIMMSTAT + U_FTPT + UBRACE + GENDER + C(UEDUC3) + CEYES + CHAIR', data=medical)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:58:06.791135Z",
     "start_time": "2020-05-03T13:58:04.734757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>U_USHRS</td>     <th>  R-squared:         </th>  <td>   0.822</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.822</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   4142.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 03 May 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:58:06</td>     <th>  Log-Likelihood:    </th> <td>-1.1026e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 35072</td>      <th>  AIC:               </th>  <td>2.206e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 35032</td>      <th>  BIC:               </th>  <td>2.209e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    39</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td>   11.2403</td> <td>    0.410</td> <td>   27.401</td> <td> 0.000</td> <td>   10.436</td> <td>   12.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married, do not live together]</th> <td>    0.8303</td> <td>    0.352</td> <td>    2.358</td> <td> 0.018</td> <td>    0.140</td> <td>    1.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married_live together]</th>         <td>    0.0955</td> <td>    0.103</td> <td>    0.923</td> <td> 0.356</td> <td>   -0.107</td> <td>    0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Never married]</th>                 <td>    0.5790</td> <td>    0.122</td> <td>    4.732</td> <td> 0.000</td> <td>    0.339</td> <td>    0.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Partnership]</th>                   <td>    0.0500</td> <td>    0.185</td> <td>    0.270</td> <td> 0.787</td> <td>   -0.312</td> <td>    0.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Separated]</th>                     <td>    0.2791</td> <td>    0.199</td> <td>    1.405</td> <td> 0.160</td> <td>   -0.110</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Unknown]</th>                       <td>   -0.6140</td> <td>    0.808</td> <td>   -0.760</td> <td> 0.447</td> <td>   -2.197</td> <td>    0.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Widowed]</th>                       <td>    0.5696</td> <td>    0.263</td> <td>    2.166</td> <td> 0.030</td> <td>    0.054</td> <td>    1.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UCUREMP[T.Yes]</th>                            <td>   -0.2474</td> <td>    0.076</td> <td>   -3.237</td> <td> 0.001</td> <td>   -0.397</td> <td>   -0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.Northeast]</th>                       <td>   -0.1911</td> <td>    0.087</td> <td>   -2.207</td> <td> 0.027</td> <td>   -0.361</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.South]</th>                           <td>   -0.0997</td> <td>    0.082</td> <td>   -1.216</td> <td> 0.224</td> <td>   -0.260</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.West]</th>                            <td>   -0.1319</td> <td>    0.084</td> <td>   -1.574</td> <td> 0.116</td> <td>   -0.296</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FHOSP[T.Yes]</th>                              <td>   -0.1287</td> <td>    0.119</td> <td>   -1.079</td> <td> 0.280</td> <td>   -0.362</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.Foreign-born, non-citizen]</th>     <td>   -0.3991</td> <td>    0.183</td> <td>   -2.180</td> <td> 0.029</td> <td>   -0.758</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.US-born citizen]</th>               <td>    0.2965</td> <td>    0.153</td> <td>    1.941</td> <td> 0.052</td> <td>   -0.003</td> <td>    0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_FTPT[T.Part-time]</th>                       <td>   -7.5472</td> <td>    0.100</td> <td>  -75.332</td> <td> 0.000</td> <td>   -7.744</td> <td>   -7.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Asian/Pacific Islander]</th>          <td>    0.0153</td> <td>    0.320</td> <td>    0.048</td> <td> 0.962</td> <td>   -0.612</td> <td>    0.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Black]</th>                           <td>    0.0104</td> <td>    0.274</td> <td>    0.038</td> <td> 0.970</td> <td>   -0.527</td> <td>    0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.White]</th>                           <td>    0.3048</td> <td>    0.270</td> <td>    1.129</td> <td> 0.259</td> <td>   -0.224</td> <td>    0.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.Male]</th>                            <td>    0.5987</td> <td>    0.066</td> <td>    9.014</td> <td> 0.000</td> <td>    0.469</td> <td>    0.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(UEDUC3)[T.1]</th>                            <td>    0.2516</td> <td>    0.105</td> <td>    2.392</td> <td> 0.017</td> <td>    0.045</td> <td>    0.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(UEDUC3)[T.2]</th>                            <td>    0.6068</td> <td>    0.116</td> <td>    5.238</td> <td> 0.000</td> <td>    0.380</td> <td>    0.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CEYES[T.brown]</th>                            <td>    0.0438</td> <td>    0.085</td> <td>    0.516</td> <td> 0.606</td> <td>   -0.122</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CEYES[T.green]</th>                            <td>    0.1628</td> <td>    0.113</td> <td>    1.441</td> <td> 0.150</td> <td>   -0.059</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CEYES[T.hazel]</th>                            <td>   -0.1141</td> <td>    0.103</td> <td>   -1.107</td> <td> 0.268</td> <td>   -0.316</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAIR[T.blond]</th>                            <td>    0.0843</td> <td>    0.114</td> <td>    0.739</td> <td> 0.460</td> <td>   -0.139</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAIR[T.brown]</th>                            <td>   -0.0475</td> <td>    0.091</td> <td>   -0.522</td> <td> 0.601</td> <td>   -0.226</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAIR[T.red]</th>                              <td>   -0.0966</td> <td>    0.123</td> <td>   -0.786</td> <td> 0.432</td> <td>   -0.338</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UAGE</th>                                      <td>   -0.0109</td> <td>    0.003</td> <td>   -3.422</td> <td> 0.001</td> <td>   -0.017</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_WKSLY</th>                                   <td>   -0.0094</td> <td>    0.003</td> <td>   -2.883</td> <td> 0.004</td> <td>   -0.016</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_HRSLY</th>                                   <td>    0.7620</td> <td>    0.003</td> <td>  258.890</td> <td> 0.000</td> <td>    0.756</td> <td>    0.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HEARNVAL</th>                                  <td>-1.488e-07</td> <td> 1.71e-06</td> <td>   -0.087</td> <td> 0.930</td> <td>-3.49e-06</td> <td> 3.19e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HOTHVAL</th>                                   <td>-6.187e-06</td> <td> 2.67e-06</td> <td>   -2.314</td> <td> 0.021</td> <td>-1.14e-05</td> <td>-9.47e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HRETVAL</th>                                   <td> 9.114e-06</td> <td> 5.77e-06</td> <td>    1.580</td> <td> 0.114</td> <td>-2.19e-06</td> <td> 2.04e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSSVAL</th>                                    <td> 7.999e-06</td> <td> 6.46e-06</td> <td>    1.237</td> <td> 0.216</td> <td>-4.67e-06</td> <td> 2.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HWSVAL</th>                                    <td> 4.514e-07</td> <td> 1.79e-06</td> <td>    0.252</td> <td> 0.801</td> <td>-3.06e-06</td> <td> 3.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDENT</th>                                     <td>    0.0038</td> <td>    0.021</td> <td>    0.186</td> <td> 0.853</td> <td>   -0.037</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FEMER</th>                                     <td>    0.0336</td> <td>    0.041</td> <td>    0.823</td> <td> 0.411</td> <td>   -0.046</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDOCT</th>                                     <td>   -0.0158</td> <td>    0.009</td> <td>   -1.775</td> <td> 0.076</td> <td>   -0.033</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>URELATE</th>                                   <td>    0.0827</td> <td>    0.024</td> <td>    3.417</td> <td> 0.001</td> <td>    0.035</td> <td>    0.130</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>15710.592</td> <th>  Durbin-Watson:     </th>  <td>   1.972</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>794998.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.400</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>26.156</td>   <th>  Cond. No.          </th>  <td>2.57e+06</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.57e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                U_USHRS   R-squared:                       0.822\n",
       "Model:                            OLS   Adj. R-squared:                  0.822\n",
       "Method:                 Least Squares   F-statistic:                     4142.\n",
       "Date:                Sun, 03 May 2020   Prob (F-statistic):               0.00\n",
       "Time:                        15:58:06   Log-Likelihood:            -1.1026e+05\n",
       "No. Observations:               35072   AIC:                         2.206e+05\n",
       "Df Residuals:                   35032   BIC:                         2.209e+05\n",
       "Df Model:                          39                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================================\n",
       "                                                coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                    11.2403      0.410     27.401      0.000      10.436      12.044\n",
       "UMARSTAT[T.Married, do not live together]     0.8303      0.352      2.358      0.018       0.140       1.520\n",
       "UMARSTAT[T.Married_live together]             0.0955      0.103      0.923      0.356      -0.107       0.298\n",
       "UMARSTAT[T.Never married]                     0.5790      0.122      4.732      0.000       0.339       0.819\n",
       "UMARSTAT[T.Partnership]                       0.0500      0.185      0.270      0.787      -0.312       0.412\n",
       "UMARSTAT[T.Separated]                         0.2791      0.199      1.405      0.160      -0.110       0.668\n",
       "UMARSTAT[T.Unknown]                          -0.6140      0.808     -0.760      0.447      -2.197       0.969\n",
       "UMARSTAT[T.Widowed]                           0.5696      0.263      2.166      0.030       0.054       1.085\n",
       "UCUREMP[T.Yes]                               -0.2474      0.076     -3.237      0.001      -0.397      -0.098\n",
       "REGION[T.Northeast]                          -0.1911      0.087     -2.207      0.027      -0.361      -0.021\n",
       "REGION[T.South]                              -0.0997      0.082     -1.216      0.224      -0.260       0.061\n",
       "REGION[T.West]                               -0.1319      0.084     -1.574      0.116      -0.296       0.032\n",
       "FHOSP[T.Yes]                                 -0.1287      0.119     -1.079      0.280      -0.362       0.105\n",
       "UIMMSTAT[T.Foreign-born, non-citizen]        -0.3991      0.183     -2.180      0.029      -0.758      -0.040\n",
       "UIMMSTAT[T.US-born citizen]                   0.2965      0.153      1.941      0.052      -0.003       0.596\n",
       "U_FTPT[T.Part-time]                          -7.5472      0.100    -75.332      0.000      -7.744      -7.351\n",
       "UBRACE[T.Asian/Pacific Islander]              0.0153      0.320      0.048      0.962      -0.612       0.643\n",
       "UBRACE[T.Black]                               0.0104      0.274      0.038      0.970      -0.527       0.548\n",
       "UBRACE[T.White]                               0.3048      0.270      1.129      0.259      -0.224       0.834\n",
       "GENDER[T.Male]                                0.5987      0.066      9.014      0.000       0.469       0.729\n",
       "C(UEDUC3)[T.1]                                0.2516      0.105      2.392      0.017       0.045       0.458\n",
       "C(UEDUC3)[T.2]                                0.6068      0.116      5.238      0.000       0.380       0.834\n",
       "CEYES[T.brown]                                0.0438      0.085      0.516      0.606      -0.122       0.210\n",
       "CEYES[T.green]                                0.1628      0.113      1.441      0.150      -0.059       0.384\n",
       "CEYES[T.hazel]                               -0.1141      0.103     -1.107      0.268      -0.316       0.088\n",
       "CHAIR[T.blond]                                0.0843      0.114      0.739      0.460      -0.139       0.308\n",
       "CHAIR[T.brown]                               -0.0475      0.091     -0.522      0.601      -0.226       0.131\n",
       "CHAIR[T.red]                                 -0.0966      0.123     -0.786      0.432      -0.338       0.144\n",
       "UAGE                                         -0.0109      0.003     -3.422      0.001      -0.017      -0.005\n",
       "U_WKSLY                                      -0.0094      0.003     -2.883      0.004      -0.016      -0.003\n",
       "U_HRSLY                                       0.7620      0.003    258.890      0.000       0.756       0.768\n",
       "HEARNVAL                                  -1.488e-07   1.71e-06     -0.087      0.930   -3.49e-06    3.19e-06\n",
       "HOTHVAL                                   -6.187e-06   2.67e-06     -2.314      0.021   -1.14e-05   -9.47e-07\n",
       "HRETVAL                                    9.114e-06   5.77e-06      1.580      0.114   -2.19e-06    2.04e-05\n",
       "HSSVAL                                     7.999e-06   6.46e-06      1.237      0.216   -4.67e-06    2.07e-05\n",
       "HWSVAL                                     4.514e-07   1.79e-06      0.252      0.801   -3.06e-06    3.96e-06\n",
       "FDENT                                         0.0038      0.021      0.186      0.853      -0.037       0.044\n",
       "FEMER                                         0.0336      0.041      0.823      0.411      -0.046       0.114\n",
       "FDOCT                                        -0.0158      0.009     -1.775      0.076      -0.033       0.002\n",
       "URELATE                                       0.0827      0.024      3.417      0.001       0.035       0.130\n",
       "==============================================================================\n",
       "Omnibus:                    15710.592   Durbin-Watson:                   1.972\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           794998.681\n",
       "Skew:                           1.400   Prob(JB):                         0.00\n",
       "Kurtosis:                      26.156   Cond. No.                     2.57e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.57e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = sm.OLS.from_formula(formula='U_USHRS ~ UAGE + U_WKSLY + U_HRSLY + HEARNVAL + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + FDENT + FEMER + FDOCT + UMARSTAT + UCUREMP + URELATE + REGION + FHOSP + UIMMSTAT + U_FTPT + UBRACE + GENDER + C(UEDUC3) + CEYES + CHAIR', data=medical)\n",
    "## we prdicted how many hours worked\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:58:14.175892Z",
     "start_time": "2020-05-03T13:58:14.066770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.82\n",
      "Mean squared error: 31.50\n",
      "Mean absolute error: 3.14\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "preds = res.predict()\n",
    "print(\"R^2: %.2f\" % metrics.r2_score(medical[\"U_USHRS\"].values, preds))\n",
    "print(\"Mean squared error: %.2f\" % metrics.mean_squared_error(medical[\"U_USHRS\"].values, preds))\n",
    "print(\"Mean absolute error: %.2f\" % metrics.mean_absolute_error(medical[\"U_USHRS\"].values, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:58:19.189982Z",
     "start_time": "2020-05-03T13:58:18.994047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdc5b3f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "medical[\"U_USHRS\"].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:58:26.023798Z",
     "start_time": "2020-05-03T13:58:25.854639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xde35fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:58:29.249710Z",
     "start_time": "2020-05-03T13:58:29.238379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared log error: 0.03\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Mean squared log error: %.2f\" % metrics.mean_squared_log_error(medical[\"U_USHRS\"].values, preds))\n",
    "except:\n",
    "    print(\"Something went wrong\")\n",
    "    \n",
    "## sometimes negative predictions - so we do it with try except function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:59:25.639679Z",
     "start_time": "2020-05-03T13:59:22.876261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>  <td>['UCURNINS[No]', 'UCURNINS[Yes]']</td> <th>  No. Observations:  </th>  <td> 35072</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                         <td>GLM</td>                <th>  Df Residuals:      </th>  <td> 35040</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>               <td>Binomial</td>              <th>  Df Model:          </th>  <td>    31</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                <td>logit</td>               <th>  Scale:             </th> <td>  1.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                       <td>IRLS</td>                <th>  Log-Likelihood:    </th> <td> -11182.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                   <td>Sun, 03 May 2020</td>          <th>  Deviance:          </th> <td>  22364.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                       <td>15:59:25</td>              <th>  Pearson chi2:      </th> <td>4.22e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                 <td>6</td>                 <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                 <td>   -3.1098</td> <td>    0.208</td> <td>  -14.947</td> <td> 0.000</td> <td>   -3.518</td> <td>   -2.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married, do not live together]</th> <td>    0.3597</td> <td>    0.163</td> <td>    2.203</td> <td> 0.028</td> <td>    0.040</td> <td>    0.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married_live together]</th>         <td>    0.8366</td> <td>    0.056</td> <td>   14.849</td> <td> 0.000</td> <td>    0.726</td> <td>    0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Never married]</th>                 <td>    0.3113</td> <td>    0.064</td> <td>    4.891</td> <td> 0.000</td> <td>    0.187</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Partnership]</th>                   <td>   -0.3488</td> <td>    0.085</td> <td>   -4.119</td> <td> 0.000</td> <td>   -0.515</td> <td>   -0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Separated]</th>                     <td>    0.0552</td> <td>    0.095</td> <td>    0.580</td> <td> 0.562</td> <td>   -0.131</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Unknown]</th>                       <td>   -0.7243</td> <td>    0.378</td> <td>   -1.916</td> <td> 0.055</td> <td>   -1.465</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Widowed]</th>                       <td>    0.1789</td> <td>    0.143</td> <td>    1.248</td> <td> 0.212</td> <td>   -0.102</td> <td>    0.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.Northeast]</th>                       <td>    0.0397</td> <td>    0.056</td> <td>    0.705</td> <td> 0.481</td> <td>   -0.071</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.South]</th>                           <td>   -0.7186</td> <td>    0.047</td> <td>  -15.412</td> <td> 0.000</td> <td>   -0.810</td> <td>   -0.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.West]</th>                            <td>   -0.4015</td> <td>    0.049</td> <td>   -8.163</td> <td> 0.000</td> <td>   -0.498</td> <td>   -0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FHOSP[T.Yes]</th>                              <td>    0.2435</td> <td>    0.075</td> <td>    3.244</td> <td> 0.001</td> <td>    0.096</td> <td>    0.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.Foreign-born, non-citizen]</th>     <td>   -0.7392</td> <td>    0.088</td> <td>   -8.367</td> <td> 0.000</td> <td>   -0.912</td> <td>   -0.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.US-born citizen]</th>               <td>    0.5805</td> <td>    0.080</td> <td>    7.284</td> <td> 0.000</td> <td>    0.424</td> <td>    0.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_FTPT[T.Part-time]</th>                       <td>   -0.4990</td> <td>    0.058</td> <td>   -8.591</td> <td> 0.000</td> <td>   -0.613</td> <td>   -0.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Asian/Pacific Islander]</th>          <td>    1.0049</td> <td>    0.160</td> <td>    6.273</td> <td> 0.000</td> <td>    0.691</td> <td>    1.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Black]</th>                           <td>    0.4877</td> <td>    0.126</td> <td>    3.881</td> <td> 0.000</td> <td>    0.241</td> <td>    0.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.White]</th>                           <td>    0.5185</td> <td>    0.117</td> <td>    4.418</td> <td> 0.000</td> <td>    0.288</td> <td>    0.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.Male]</th>                            <td>   -0.1578</td> <td>    0.039</td> <td>   -4.099</td> <td> 0.000</td> <td>   -0.233</td> <td>   -0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED</th>                                   <td>    0.3114</td> <td>    0.015</td> <td>   20.834</td> <td> 0.000</td> <td>    0.282</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>URELATE</th>                                   <td>    0.0093</td> <td>    0.013</td> <td>    0.704</td> <td> 0.481</td> <td>   -0.017</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDENT</th>                                     <td>    0.3195</td> <td>    0.015</td> <td>   21.437</td> <td> 0.000</td> <td>    0.290</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FEMER</th>                                     <td>   -0.0773</td> <td>    0.022</td> <td>   -3.519</td> <td> 0.000</td> <td>   -0.120</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDOCT</th>                                     <td>    0.1418</td> <td>    0.008</td> <td>   17.051</td> <td> 0.000</td> <td>    0.126</td> <td>    0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UAGE</th>                                      <td>    0.0180</td> <td>    0.002</td> <td>    9.736</td> <td> 0.000</td> <td>    0.014</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_WKSLY</th>                                   <td>    0.0198</td> <td>    0.002</td> <td>   12.862</td> <td> 0.000</td> <td>    0.017</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_USHRS</th>                                   <td>   -0.0021</td> <td>    0.002</td> <td>   -1.160</td> <td> 0.246</td> <td>   -0.006</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HOTHVAL</th>                                   <td> 3.526e-07</td> <td> 1.57e-06</td> <td>    0.225</td> <td> 0.822</td> <td>-2.72e-06</td> <td> 3.43e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HRETVAL</th>                                   <td>-5.481e-06</td> <td> 3.18e-06</td> <td>   -1.724</td> <td> 0.085</td> <td>-1.17e-05</td> <td> 7.49e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSSVAL</th>                                    <td> 5.403e-06</td> <td> 3.76e-06</td> <td>    1.436</td> <td> 0.151</td> <td>-1.97e-06</td> <td> 1.28e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HWSVAL</th>                                    <td>-3.837e-09</td> <td> 3.23e-07</td> <td>   -0.012</td> <td> 0.991</td> <td>-6.38e-07</td> <td>  6.3e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3</th>                                    <td>    0.7876</td> <td>    0.031</td> <td>   25.467</td> <td> 0.000</td> <td>    0.727</td> <td>    0.848</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                         Generalized Linear Model Regression Results                         \n",
       "=============================================================================================\n",
       "Dep. Variable:     ['UCURNINS[No]', 'UCURNINS[Yes]']   No. Observations:                35072\n",
       "Model:                                           GLM   Df Residuals:                    35040\n",
       "Model Family:                               Binomial   Df Model:                           31\n",
       "Link Function:                                 logit   Scale:                          1.0000\n",
       "Method:                                         IRLS   Log-Likelihood:                -11182.\n",
       "Date:                               Sun, 03 May 2020   Deviance:                       22364.\n",
       "Time:                                       15:59:25   Pearson chi2:                 4.22e+04\n",
       "No. Iterations:                                    6   Covariance Type:             nonrobust\n",
       "=============================================================================================================\n",
       "                                                coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                    -3.1098      0.208    -14.947      0.000      -3.518      -2.702\n",
       "UMARSTAT[T.Married, do not live together]     0.3597      0.163      2.203      0.028       0.040       0.680\n",
       "UMARSTAT[T.Married_live together]             0.8366      0.056     14.849      0.000       0.726       0.947\n",
       "UMARSTAT[T.Never married]                     0.3113      0.064      4.891      0.000       0.187       0.436\n",
       "UMARSTAT[T.Partnership]                      -0.3488      0.085     -4.119      0.000      -0.515      -0.183\n",
       "UMARSTAT[T.Separated]                         0.0552      0.095      0.580      0.562      -0.131       0.242\n",
       "UMARSTAT[T.Unknown]                          -0.7243      0.378     -1.916      0.055      -1.465       0.017\n",
       "UMARSTAT[T.Widowed]                           0.1789      0.143      1.248      0.212      -0.102       0.460\n",
       "REGION[T.Northeast]                           0.0397      0.056      0.705      0.481      -0.071       0.150\n",
       "REGION[T.South]                              -0.7186      0.047    -15.412      0.000      -0.810      -0.627\n",
       "REGION[T.West]                               -0.4015      0.049     -8.163      0.000      -0.498      -0.305\n",
       "FHOSP[T.Yes]                                  0.2435      0.075      3.244      0.001       0.096       0.391\n",
       "UIMMSTAT[T.Foreign-born, non-citizen]        -0.7392      0.088     -8.367      0.000      -0.912      -0.566\n",
       "UIMMSTAT[T.US-born citizen]                   0.5805      0.080      7.284      0.000       0.424       0.737\n",
       "U_FTPT[T.Part-time]                          -0.4990      0.058     -8.591      0.000      -0.613      -0.385\n",
       "UBRACE[T.Asian/Pacific Islander]              1.0049      0.160      6.273      0.000       0.691       1.319\n",
       "UBRACE[T.Black]                               0.4877      0.126      3.881      0.000       0.241       0.734\n",
       "UBRACE[T.White]                               0.5185      0.117      4.418      0.000       0.288       0.748\n",
       "GENDER[T.Male]                               -0.1578      0.039     -4.099      0.000      -0.233      -0.082\n",
       "USATMED                                       0.3114      0.015     20.834      0.000       0.282       0.341\n",
       "URELATE                                       0.0093      0.013      0.704      0.481      -0.017       0.035\n",
       "FDENT                                         0.3195      0.015     21.437      0.000       0.290       0.349\n",
       "FEMER                                        -0.0773      0.022     -3.519      0.000      -0.120      -0.034\n",
       "FDOCT                                         0.1418      0.008     17.051      0.000       0.126       0.158\n",
       "UAGE                                          0.0180      0.002      9.736      0.000       0.014       0.022\n",
       "U_WKSLY                                       0.0198      0.002     12.862      0.000       0.017       0.023\n",
       "U_USHRS                                      -0.0021      0.002     -1.160      0.246      -0.006       0.001\n",
       "HOTHVAL                                    3.526e-07   1.57e-06      0.225      0.822   -2.72e-06    3.43e-06\n",
       "HRETVAL                                   -5.481e-06   3.18e-06     -1.724      0.085   -1.17e-05    7.49e-07\n",
       "HSSVAL                                     5.403e-06   3.76e-06      1.436      0.151   -1.97e-06    1.28e-05\n",
       "HWSVAL                                    -3.837e-09   3.23e-07     -0.012      0.991   -6.38e-07     6.3e-07\n",
       "UEDUC3                                        0.7876      0.031     25.467      0.000       0.727       0.848\n",
       "=============================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Classification problem\n",
    "mod = sm.GLM.from_formula(formula=\"UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + UEDUC3 + GENDER\", data=medical, family=sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:59:30.013862Z",
     "start_time": "2020-05-03T13:59:29.993910Z"
    }
   },
   "outputs": [],
   "source": [
    "probs = res.predict()\n",
    "# Statsmodels took No as 1 due to alphabetic sorting. Hence >\n",
    "preds = np.array([1 if x<0.5 else 0 for x in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:59:31.113798Z",
     "start_time": "2020-05-03T13:59:31.066410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCURNINS</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29291</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3843</td>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0         0     1\n",
       "UCURNINS             \n",
       "0         29291   715\n",
       "1          3843  1223"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab((medical.UCURNINS==\"Yes\").astype(int), preds)\n",
    "## many options in pandas crosstab - normalize, margins, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T13:59:59.309152Z",
     "start_time": "2020-05-03T13:59:59.113660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xddf9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve((medical.UCURNINS==\"No\").astype(int), probs)\n",
    "## third value - '_' - different levels of treshhold but we dont need it for plotting\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T14:00:02.393936Z",
     "start_time": "2020-05-03T14:00:02.080212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5f67850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score((medical.UCURNINS==\"No\").astype(int), probs)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve((medical.UCURNINS==\"No\").astype(int), probs)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.8,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.25,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic passengers data – 1310 observations and 15 variables:\n",
    "\n",
    "# passenger_id – Unique passenger id\n",
    "# pclass – Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "# survived – Survival (0 = No, 1 = Yes)\n",
    "# name – Name and SUrname\n",
    "# sex – Sex (0 = Male, 1 = Female)\n",
    "# age – Age in years\n",
    "# sibsp – # of siblings / spouses aboard the Titanic\n",
    "# parch – # of parents / children aboard the Titanic\n",
    "# ticket – Ticket number\n",
    "# fare – Passenger fare\n",
    "# cabin – Cabin number\n",
    "# embarked – Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "# boat – Lifeboat (if survived)\n",
    "# body – Body number (if did not survive and body was recovered)\n",
    "# home.dest – Home/Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For you best models from multinominal logistic regression and LDE\n",
    "# Prepare Confusion matrix\n",
    "# Plot a ROC Curve (with AUC)\n",
    "# Plot Precision Recall Curve (with AUC)\n",
    "# Calculate log loss, accuracy and balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['passenger_id', 'pclass', 'survived', 'name', 'sex', 'age', 'sibsp',\n",
       "       'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body',\n",
       "       'home.dest'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-954cb434ea37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Classification problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_formula\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"survived ~ pclass + + sex + age + sibsp + parch + ticket + fare + cabin + embarked + boat + body\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[1;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m                        \u001b[1;34m'formula'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mformula\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# attach formula for unpckling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                        'design_info': design_info})\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformula\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformula\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, family, offset, exposure, freq_weights, var_weights, missing, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m                                   \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexposure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexposure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                                   \u001b[0mfreq_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreq_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                                   var_weights=var_weights, **kwargs)\n\u001b[0m\u001b[0;32m    321\u001b[0m         self._check_inputs(family, self.offset, self.exposure, self.endog,\n\u001b[0;32m    322\u001b[0m                            self.freq_weights, self.var_weights)\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hasconst'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[1;32m---> 64\u001b[1;33m                                       **kwargs)\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[1;32m--> 633\u001b[1;33m                  **kwargs)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresettable_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michal_schudnij\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\data.py\u001b[0m in \u001b[0;36m_handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;31m# detect where the constant is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mcheck_implicit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mptp_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptp_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'exog contains inf or nans'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "## Classification problem\n",
    "mod = sm.MNLogit.from_formula(\"survived ~ pclass + + sex + age + sibsp + parch + ticket + fare + cabin + embarked + boat + body\", data=titanic)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.genmod.generalized_linear_model.GLM at 0x3d00a10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = res.predict()\n",
    "# Statsmodels took No as 1 due to alphabetic sorting. Hence >\n",
    "preds = np.array([1 if x<0.5 else 0 for x in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab((medical.UCURNINS==\"Yes\").astype(int), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality Data Set: \"data/wines.csv\"\n",
    "# source: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "# The file contains data on samples of white and red Portuguese wine \n",
    "# Vinho Verde. \n",
    "# Various physico-chemical characteristics of individual samples\n",
    "# are available as well as wine quality scores on a point scale (0-10) \n",
    "# made by specialists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For you best models from multinominal logistic regression and LDE\n",
    "# calcualte TPR and PPV for every class\n",
    "# calcualte multinominal logloss for your best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = pd.read_csv(\"data/wines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>type</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>red</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>red</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.097</td>\n",
       "      <td>15.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99430</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.114</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.176</td>\n",
       "      <td>52.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.170</td>\n",
       "      <td>51.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.092</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.28</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.086</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>red</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.341</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>red</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.077</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.082</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.084</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.085</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>red</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.080</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.080</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>red</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.082</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>red</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6467</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.31</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.046</td>\n",
       "      <td>42.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.99324</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6468</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10.10</td>\n",
       "      <td>0.032</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99626</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6469</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.28</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.021</td>\n",
       "      <td>29.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99188</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.36</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6470</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98970</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.55</td>\n",
       "      <td>12.050000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6471</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.41</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.032</td>\n",
       "      <td>50.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.030</td>\n",
       "      <td>33.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.99044</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.99282</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6474</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99245</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.41</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.038</td>\n",
       "      <td>34.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.99132</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.59</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6476</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.032</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.99286</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6477</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.035</td>\n",
       "      <td>6.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99234</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.35</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>white</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6478</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.533333</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6479</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.533333</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6480</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.27</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6481</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.037</td>\n",
       "      <td>45.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.99184</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6482</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.035</td>\n",
       "      <td>60.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.98964</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6483</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.048</td>\n",
       "      <td>68.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.99492</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6484</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.550000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6485</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.028</td>\n",
       "      <td>45.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99168</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.08</td>\n",
       "      <td>12.150000</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6486</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.98928</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.79</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6487</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.052</td>\n",
       "      <td>38.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.99330</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6488</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.27</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.036</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.98938</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>38.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99074</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>white</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>white</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00            1.90      0.076   \n",
       "1               7.8             0.880         0.00            2.60      0.098   \n",
       "2               7.8             0.760         0.04            2.30      0.092   \n",
       "3              11.2             0.280         0.56            1.90      0.075   \n",
       "4               7.4             0.700         0.00            1.90      0.076   \n",
       "5               7.4             0.660         0.00            1.80      0.075   \n",
       "6               7.9             0.600         0.06            1.60      0.069   \n",
       "7               7.3             0.650         0.00            1.20      0.065   \n",
       "8               7.8             0.580         0.02            2.00      0.073   \n",
       "9               7.5             0.500         0.36            6.10      0.071   \n",
       "10              6.7             0.580         0.08            1.80      0.097   \n",
       "11              7.5             0.500         0.36            6.10      0.071   \n",
       "12              5.6             0.615         0.00            1.60      0.089   \n",
       "13              7.8             0.610         0.29            1.60      0.114   \n",
       "14              8.9             0.620         0.18            3.80      0.176   \n",
       "15              8.9             0.620         0.19            3.90      0.170   \n",
       "16              8.5             0.280         0.56            1.80      0.092   \n",
       "17              8.1             0.560         0.28            1.70      0.368   \n",
       "18              7.4             0.590         0.08            4.40      0.086   \n",
       "19              7.9             0.320         0.51            1.80      0.341   \n",
       "20              8.9             0.220         0.48            1.80      0.077   \n",
       "21              7.6             0.390         0.31            2.30      0.082   \n",
       "22              7.9             0.430         0.21            1.60      0.106   \n",
       "23              8.5             0.490         0.11            2.30      0.084   \n",
       "24              6.9             0.400         0.14            2.40      0.085   \n",
       "25              6.3             0.390         0.16            1.40      0.080   \n",
       "26              7.6             0.410         0.24            1.80      0.080   \n",
       "27              7.9             0.430         0.21            1.60      0.106   \n",
       "28              7.1             0.710         0.00            1.90      0.080   \n",
       "29              7.8             0.645         0.00            2.00      0.082   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6467            5.8             0.230         0.31            4.50      0.046   \n",
       "6468            6.6             0.240         0.33           10.10      0.032   \n",
       "6469            6.1             0.320         0.28            6.60      0.021   \n",
       "6470            5.0             0.200         0.40            1.90      0.015   \n",
       "6471            6.0             0.420         0.41           12.40      0.032   \n",
       "6472            5.7             0.210         0.32            1.60      0.030   \n",
       "6473            5.6             0.200         0.36            2.50      0.048   \n",
       "6474            7.4             0.220         0.26            1.20      0.035   \n",
       "6475            6.2             0.380         0.42            2.50      0.038   \n",
       "6476            5.9             0.540         0.00            0.80      0.032   \n",
       "6477            6.2             0.530         0.02            0.90      0.035   \n",
       "6478            6.6             0.340         0.40            8.10      0.046   \n",
       "6479            6.6             0.340         0.40            8.10      0.046   \n",
       "6480            5.0             0.235         0.27           11.75      0.030   \n",
       "6481            5.5             0.320         0.13            1.30      0.037   \n",
       "6482            4.9             0.470         0.17            1.90      0.035   \n",
       "6483            6.5             0.330         0.38            8.30      0.048   \n",
       "6484            6.6             0.340         0.40            8.10      0.046   \n",
       "6485            6.2             0.210         0.28            5.70      0.028   \n",
       "6486            6.2             0.410         0.22            1.90      0.023   \n",
       "6487            6.8             0.220         0.36            1.20      0.052   \n",
       "6488            4.9             0.235         0.27           11.75      0.030   \n",
       "6489            6.1             0.340         0.29            2.20      0.036   \n",
       "6490            5.7             0.210         0.32            0.90      0.038   \n",
       "6491            6.5             0.230         0.38            1.30      0.032   \n",
       "6492            6.2             0.210         0.29            1.60      0.039   \n",
       "6493            6.6             0.320         0.36            8.00      0.047   \n",
       "6494            6.5             0.240         0.19            1.20      0.041   \n",
       "6495            5.5             0.290         0.30            1.10      0.022   \n",
       "6496            6.0             0.210         0.38            0.80      0.020   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
       "6                    15.0                  59.0  0.99640  3.30       0.46   \n",
       "7                    15.0                  21.0  0.99460  3.39       0.47   \n",
       "8                     9.0                  18.0  0.99680  3.36       0.57   \n",
       "9                    17.0                 102.0  0.99780  3.35       0.80   \n",
       "10                   15.0                  65.0  0.99590  3.28       0.54   \n",
       "11                   17.0                 102.0  0.99780  3.35       0.80   \n",
       "12                   16.0                  59.0  0.99430  3.58       0.52   \n",
       "13                    9.0                  29.0  0.99740  3.26       1.56   \n",
       "14                   52.0                 145.0  0.99860  3.16       0.88   \n",
       "15                   51.0                 148.0  0.99860  3.17       0.93   \n",
       "16                   35.0                 103.0  0.99690  3.30       0.75   \n",
       "17                   16.0                  56.0  0.99680  3.11       1.28   \n",
       "18                    6.0                  29.0  0.99740  3.38       0.50   \n",
       "19                   17.0                  56.0  0.99690  3.04       1.08   \n",
       "20                   29.0                  60.0  0.99680  3.39       0.53   \n",
       "21                   23.0                  71.0  0.99820  3.52       0.65   \n",
       "22                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "23                    9.0                  67.0  0.99680  3.17       0.53   \n",
       "24                   21.0                  40.0  0.99680  3.43       0.63   \n",
       "25                   11.0                  23.0  0.99550  3.34       0.56   \n",
       "26                    4.0                  11.0  0.99620  3.28       0.59   \n",
       "27                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "28                   14.0                  35.0  0.99720  3.47       0.55   \n",
       "29                    8.0                  16.0  0.99640  3.38       0.59   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "6467                 42.0                 124.0  0.99324  3.31       0.64   \n",
       "6468                  8.0                  81.0  0.99626  3.19       0.51   \n",
       "6469                 29.0                 132.0  0.99188  3.15       0.36   \n",
       "6470                 20.0                  98.0  0.98970  3.37       0.55   \n",
       "6471                 50.0                 179.0  0.99622  3.14       0.60   \n",
       "6472                 33.0                 122.0  0.99044  3.33       0.52   \n",
       "6473                 16.0                 125.0  0.99282  3.49       0.49   \n",
       "6474                 18.0                  97.0  0.99245  3.12       0.41   \n",
       "6475                 34.0                 117.0  0.99132  3.36       0.59   \n",
       "6476                 12.0                  82.0  0.99286  3.25       0.36   \n",
       "6477                  6.0                  81.0  0.99234  3.24       0.35   \n",
       "6478                 68.0                 170.0  0.99494  3.15       0.50   \n",
       "6479                 68.0                 170.0  0.99494  3.15       0.50   \n",
       "6480                 34.0                 118.0  0.99540  3.07       0.50   \n",
       "6481                 45.0                 156.0  0.99184  3.26       0.38   \n",
       "6482                 60.0                 148.0  0.98964  3.27       0.35   \n",
       "6483                 68.0                 174.0  0.99492  3.14       0.50   \n",
       "6484                 68.0                 170.0  0.99494  3.15       0.50   \n",
       "6485                 45.0                 121.0  0.99168  3.21       1.08   \n",
       "6486                  5.0                  56.0  0.98928  3.04       0.79   \n",
       "6487                 38.0                 127.0  0.99330  3.04       0.54   \n",
       "6488                 34.0                 118.0  0.99540  3.07       0.50   \n",
       "6489                 25.0                 100.0  0.98938  3.06       0.44   \n",
       "6490                 38.0                 121.0  0.99074  3.24       0.46   \n",
       "6491                 29.0                 112.0  0.99298  3.29       0.54   \n",
       "6492                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "        alcohol   type  quality  \n",
       "0      9.400000    red        5  \n",
       "1      9.800000    red        5  \n",
       "2      9.800000    red        5  \n",
       "3      9.800000    red        6  \n",
       "4      9.400000    red        5  \n",
       "5      9.400000    red        5  \n",
       "6      9.400000    red        5  \n",
       "7     10.000000    red        7  \n",
       "8      9.500000    red        7  \n",
       "9     10.500000    red        5  \n",
       "10     9.200000    red        5  \n",
       "11    10.500000    red        5  \n",
       "12     9.900000    red        5  \n",
       "13     9.100000    red        5  \n",
       "14     9.200000    red        5  \n",
       "15     9.200000    red        5  \n",
       "16    10.500000    red        7  \n",
       "17     9.300000    red        5  \n",
       "18     9.000000    red        4  \n",
       "19     9.200000    red        6  \n",
       "20     9.400000    red        6  \n",
       "21     9.700000    red        5  \n",
       "22     9.500000    red        5  \n",
       "23     9.400000    red        5  \n",
       "24     9.700000    red        6  \n",
       "25     9.300000    red        5  \n",
       "26     9.500000    red        5  \n",
       "27     9.500000    red        5  \n",
       "28     9.400000    red        5  \n",
       "29     9.800000    red        6  \n",
       "...         ...    ...      ...  \n",
       "6467  10.800000  white        6  \n",
       "6468   9.800000  white        6  \n",
       "6469  11.450000  white        7  \n",
       "6470  12.050000  white        6  \n",
       "6471   9.700000  white        5  \n",
       "6472  11.900000  white        6  \n",
       "6473  10.000000  white        6  \n",
       "6474   9.700000  white        6  \n",
       "6475  11.600000  white        7  \n",
       "6476   8.800000  white        5  \n",
       "6477   9.500000  white        4  \n",
       "6478   9.533333  white        6  \n",
       "6479   9.533333  white        6  \n",
       "6480   9.400000  white        6  \n",
       "6481  10.700000  white        5  \n",
       "6482  11.500000  white        6  \n",
       "6483   9.600000  white        5  \n",
       "6484   9.550000  white        6  \n",
       "6485  12.150000  white        7  \n",
       "6486  13.000000  white        7  \n",
       "6487   9.200000  white        5  \n",
       "6488   9.400000  white        6  \n",
       "6489  11.800000  white        6  \n",
       "6490  10.600000  white        6  \n",
       "6491   9.700000  white        5  \n",
       "6492  11.200000  white        6  \n",
       "6493   9.600000  white        5  \n",
       "6494   9.400000  white        6  \n",
       "6495  12.800000  white        7  \n",
       "6496  11.800000  white        6  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
